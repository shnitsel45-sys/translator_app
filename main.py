import json
from typing import Dict, List, Tuple, Optional
import re
from deep_translator import GoogleTranslator
import os


class AdvancedTransformationalTranslator:
    def __init__(self):
        self.MORPH_AVAILABLE = False
        self.STANZA_AVAILABLE = False

        try:
            import pymorphy2
            import stanza
            self.MORPH_AVAILABLE = True
            self.STANZA_AVAILABLE = True
            print("‚úÖ –í—Å–µ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã –¥–æ—Å—Ç—É–ø–Ω—ã")
        except ImportError as e:
            print(f"‚ö†Ô∏è –ù–µ –≤—Å–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: {e}")
            self.MORPH_AVAILABLE = False
            self.STANZA_AVAILABLE = False

        self.translator = GoogleTranslator(source='ru', target='en')
        self.translation_cache = {}

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        self.correction_dict = {
            '—è': 'i',
            '—Ç—ã': 'you',
            '–æ–Ω': 'he',
            '–æ–Ω–∞': 'she',
            '–æ–Ω–æ': 'it',
            '–º—ã': 'we',
            '–≤—ã': 'you',
            '–æ–Ω–∏': 'they',
            '—á—Ç–æ': 'what',
            '–∫—Ç–æ': 'who',
            '–≥–¥–µ': 'where',
            '–∫–æ–≥–¥–∞': 'when',
            '–ø–æ—á–µ–º—É': 'why',
            '–∫–∞–∫': 'how',
            '—Å–∫–æ–ª—å–∫–æ': 'how much'
        }

        self.grammar_rules = self.load_grammar_rules()
        self.morph = None
        self.stanza_nlp = None

        if self.MORPH_AVAILABLE:
            try:
                import pymorphy2
                self.morph = pymorphy2.MorphAnalyzer()
                print("‚úÖ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ pymorphy2: {e}")
                self.MORPH_AVAILABLE = False

        if self.STANZA_AVAILABLE:
            try:
                import stanza
                try:
                    self.stanza_nlp = stanza.Pipeline('ru', processors='tokenize,pos,lemma,depparse')
                    print("‚úÖ Stanza —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                except:
                    print("üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å Stanza –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞...")
                    stanza.download('ru')
                    self.stanza_nlp = stanza.Pipeline('ru', processors='tokenize,pos,lemma,depparse')
                    print("‚úÖ Stanza —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Stanza: {e}")
                self.STANZA_AVAILABLE = False

        if self.is_json_loaded_properly():
            print("‚úÖ –§–∞–π–ª JSON —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —Ñ–∞–π–ª–∞.")
            self.json_loaded = True
            self.patterns = self.grammar_rules["english_grammar_system"]["sentence_patterns"]["basic_clause_patterns"]
            self.extended_patterns = self.grammar_rules["english_grammar_system"]["sentence_patterns"][
                "extended_patterns"]
            self.lexical_corrections = self.grammar_rules["english_grammar_system"].get("lexical_corrections", {})
            self.preposition_rules = self.grammar_rules["english_grammar_system"].get("preposition_rules", {})
        else:
            print("‚ö†Ô∏è –§–∞–π–ª JSON –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞.")
            self.json_loaded = False
            self.patterns = {}
            self.extended_patterns = {}
            self.lexical_corrections = {}
            self.preposition_rules = {}

    def is_json_loaded_properly(self) -> bool:
        if not self.grammar_rules or "error" in self.grammar_rules:
            return False
        return "english_grammar_system" in self.grammar_rules

    def load_grammar_rules(self) -> Dict:
        json_files = ['transformational_grammar_rules.json']
        for json_file in json_files:
            try:
                if os.path.exists(json_file):
                    print(f"üîç –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {json_file}")
                    with open(json_file, 'r', encoding='utf-8') as f:
                        rules = json.load(f)
                        print(f"‚úÖ –§–∞–π–ª {json_file} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
                        return rules
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {json_file}: {e}")
        return {"error": "No JSON file found"}

    def stanza_syntax_analysis(self, sentence: str) -> Dict:
        if not self.STANZA_AVAILABLE or not self.stanza_nlp:
            return {"error": "Stanza analyzer not available"}
        try:
            doc = self.stanza_nlp(sentence)
            analysis = {"sentences": [], "tokens": [], "dependencies": []}
            for sent in doc.sentences:
                sentence_info = {"text": sent.text, "words": [], "dependencies": []}
                for word in sent.words:
                    word_info = {
                        "id": word.id,
                        "text": word.text,
                        "lemma": word.lemma,
                        "upos": word.upos,
                        "xpos": word.xpos,
                        "feats": word.feats,
                        "head": word.head,
                        "deprel": word.deprel
                    }
                    sentence_info["words"].append(word_info)
                    analysis["tokens"].append(word_info)
                for word in sent.words:
                    if word.head > 0:
                        dependency = {
                            "governor": word.head,
                            "dependent": word.id,
                            "relation": word.deprel
                        }
                        sentence_info["dependencies"].append(dependency)
                        analysis["dependencies"].append(dependency)
                analysis["sentences"].append(sentence_info)
            return analysis
        except Exception as e:
            return {"error": str(e)}

    def find_subject_with_stanza(self, analysis: Dict) -> Tuple[str, str]:
        if "error" in analysis or not analysis.get("sentences"):
            return None, None
        subject = None
        predicate = None
        for sentence in analysis["sentences"]:
            for word in sentence["words"]:
                if word["deprel"] == "nsubj":
                    subject = word["text"]
                elif word["deprel"] == "root" and word["upos"] == "VERB":
                    predicate = word["text"]
                elif word["deprel"] in ["nsubj:pass", "csubj"]:
                    subject = word["text"]
            if not subject or not predicate:
                for word in sentence["words"]:
                    if word["upos"] == "NOUN" and word["deprel"] != "obj":
                        subject = word["text"]
                    elif word["upos"] == "VERB" and not predicate:
                        predicate = word["text"]
        return subject, predicate

    def find_objects_with_stanza(self, analysis: Dict) -> Tuple[str, str]:
        if "error" in analysis or not analysis.get("sentences"):
            return None, None
        direct_object = None
        indirect_object = None
        for sentence in analysis["sentences"]:
            for word in sentence["words"]:
                if word["deprel"] == "obj":
                    direct_object = word["text"]
                elif word["deprel"] == "iobj":
                    indirect_object = word["text"]
                elif word["deprel"] in ["obl", "nmod"]:
                    if not direct_object:
                        direct_object = word["text"]
        return direct_object, indirect_object

    def advanced_syntax_analysis_with_stanza(self, structure: Dict, sentence: str):
        stanza_analysis = self.stanza_syntax_analysis(sentence)
        if "error" not in stanza_analysis:
            structure["stanza_analysis"] = stanza_analysis
            subject, predicate = self.find_subject_with_stanza(stanza_analysis)
            direct_obj, indirect_obj = self.find_objects_with_stanza(stanza_analysis)
            if subject:
                structure["subject"] = subject
            if predicate:
                structure["verb"] = predicate
            if direct_obj:
                structure["object"] = direct_obj
            if indirect_obj:
                structure["indirect_object"] = indirect_obj
            return
        self.fallback_syntax_analysis(structure, sentence)

    def fallback_syntax_analysis(self, structure: Dict, sentence: str):
        clean_sentence = re.sub(r'[^\w\s]', ' ', sentence)
        words = [word for word in clean_sentence.strip().split() if word]
        question_words = ['—á—Ç–æ', '–∫—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–∞–∫', '—Å–∫–æ–ª—å–∫–æ']
        for word in words:
            if word.lower() in question_words:
                structure["question_word"] = word
                break
        for word in words:
            if self.is_verb_simple(word):
                structure["verb"] = word
                break
        for word in words:
            if word.lower() in ['—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º—ã', '–≤—ã', '–æ–Ω–∏']:
                structure["subject"] = word
                break
        if structure.get("verb"):
            verb_index = words.index(structure["verb"])
            for i in range(verb_index + 1, len(words)):
                word = words[i]
                if word != structure.get("subject") and not self.is_verb_simple(word):
                    structure["object"] = word
                    break

    def is_verb_simple(self, word: str) -> bool:
        if self.MORPH_AVAILABLE and self.morph:
            parses = self.morph.parse(word)
            for p in parses:
                if p.tag.POS in {'VERB', 'INFN'}:
                    return True
        word_lower = word.lower()
        verb_endings = [
            '—Ç—å', '–µ—Ç', '—ë—Ç', '–∏—Ç', '–∞—Ç', '—è—Ç',
            '–µ—à—å', '—ë—à—å', '–∏—à—å', '–∞—à—å', '—è—à—å',
            '–µ–º', '—ë–º', '–∏–º', '–∞–º', '—è–º',
            '–µ—Ç–µ', '—ë—Ç–µ', '–∏—Ç–µ', '–∞—Ç–µ', '—è—Ç–µ',
            '—É—Ç', '—é—Ç'
        ]
        special_verbs = ['–µ—Å—Ç—å', '–±—ã—Ç—å', '—è–≤–ª—è—Ç—å—Å—è', '—Å—Ç–∞—Ç—å', '–∫–∞–∑–∞—Ç—å—Å—è']
        return any(word_lower.endswith(ending) for ending in verb_endings) or word_lower in special_verbs

    def _safe_translate_word(self, word: str) -> str:
        if word in self.translation_cache:
            return self.translation_cache[word]
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ JSON
            if self.json_loaded and self.lexical_corrections:
                for category in self.lexical_corrections.values():
                    if word.lower() in category:
                        result = category[word.lower()]
                        self.translation_cache[word] = result
                        return result

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º correction_dict
            word_lower = word.lower()
            if word_lower in self.correction_dict:
                result = self.correction_dict[word_lower]
                self.translation_cache[word] = result
                return result

            # –û–±—ã—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
            raw = self.translator.translate(word).lower().strip()
            if ' ' in raw:
                stop_words = {'i', 'you', 'he', 'she', 'it', 'we', 'they',
                              'am', 'is', 'are', 'was', 'were',
                              'do', 'does', 'did',
                              'have', 'has', 'had',
                              'be', 'been', 'being'}
                candidates = [w for w in raw.split() if w not in stop_words]
                result = candidates[0] if candidates else word
            else:
                result = raw
            self.translation_cache[word] = result
            return result
        except:
            self.translation_cache[word] = word
            return word

    def get_verb_info(self, russian_verb: str) -> dict:
        if self.MORPH_AVAILABLE and self.morph:
            parses = self.morph.parse(russian_verb)
            for p in parses:
                if p.tag.POS in {'VERB', 'INFN'}:
                    normal_form = p.normal_form
                    tense = getattr(p.tag, 'tense', None)
                    return {"infinitive": normal_form, "tense": tense}
        return {"infinitive": russian_verb, "tense": None}

    def get_word_translation_improved(self, word: str) -> str:
        if self.MORPH_AVAILABLE and self.morph:
            parses = self.morph.parse(word)
            if parses:
                p = parses[0]
                normal_form = p.normal_form
                return self._safe_translate_word(normal_form)
        return self._safe_translate_word(word)

    def detect_special_pattern(self, sentence: str, structure: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ —à–∞–±–ª–æ–Ω–∞–º –∏–∑ JSON"""
        sentence_lower = sentence.lower()

        # 1. POSSESSION: "–£ –º–µ–Ω—è –µ—Å—Ç—å..."
        if self.extended_patterns and "POSSESSION" in self.extended_patterns:
            possession_pattern = self.extended_patterns["POSSESSION"].get("trigger_regex")
            if possession_pattern and re.search(possession_pattern, sentence_lower):
                return "POSSESSION"

        # 2. PRICE_QUESTION: "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç..."
        if self.extended_patterns and "PRICE_QUESTION" in self.extended_patterns:
            price_pattern = self.extended_patterns["PRICE_QUESTION"].get("trigger_regex")
            if price_pattern and re.search(price_pattern, sentence_lower):
                return "PRICE_QUESTION"

        # 3. PHRASAL_VERB: "–∂–¥–∞—Ç—å", "—Å–º–æ—Ç—Ä–µ—Ç—å", "–∏–¥—Ç–∏"
        if self.extended_patterns and "PHRASAL_VERB" in self.extended_patterns:
            trigger_verbs = self.extended_patterns["PHRASAL_VERB"].get("trigger_verbs", [])
            if any(verb in sentence_lower for verb in trigger_verbs):
                return "PHRASAL_VERB"

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
        has_subject = structure.get("subject") is not None
        has_verb = structure.get("verb") is not None
        has_object = structure.get("object") is not None
        has_indirect = structure.get("indirect_object") is not None

        adverbial_found = False
        prepositions = {'–≤', '–≤–æ', '–Ω–∞', '—É', '—Å', '–∫', '–∏–∑', '–±–µ–∑', '–æ', '–æ–±', '–∑–∞', '–ø–æ'}
        for word in structure["words_ru"]:
            if word.lower() in prepositions:
                adverbial_found = True
                break

        if not has_subject or not has_verb:
            return "SV"

        verb = structure.get("verb", "").lower()
        dative_verbs = {"–¥–∞–ª", "–¥–∞–ª–∞", "–¥–∞–ª–æ", "–¥–∞–ª–∏", "–¥–∞—Ç—å", "–ø–æ–¥–∞—Ä–∏–ª", "–ø–æ–¥–∞—Ä–∏–ª–∞", "–æ—Ç–¥–∞–ª", "–æ—Ç–¥–∞–ª–∞", "–ø–µ—Ä–µ–¥–∞–ª",
                        "–ø–µ—Ä–µ–¥–∞–ª–∞"}
        if has_indirect and has_object and verb in dative_verbs:
            return "SVOO"

        if has_object and adverbial_found:
            return "SVOA"
        elif has_object:
            return "SVO"
        elif adverbial_found:
            return "SVA"
        else:
            return "SV"

    def determine_article(self, structure: Dict, word_ru: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞—Ä—Ç–∏–∫–ª—å –¥–ª—è —Å–ª–æ–≤–∞"""
        word_lower = word_ru.lower()

        # –ù–µ–∏—Å—á–∏—Å–ª—è–µ–º—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ ‚Äî –±–µ–∑ –∞—Ä—Ç–∏–∫–ª—è
        uncountable = {"—á–∞–π", "–∫–æ—Ñ–µ", "–º–æ–ª–æ–∫–æ", "–≤–æ–¥–∞", "–¥–æ–º–∞", "—Ä–∞–±–æ—Ç—ã"}
        if word_lower in uncountable:
            return ""

        # –ü–æ—Å–ª–µ –ø—Ä–µ–¥–ª–æ–≥–æ–≤ ‚Äî –≤—Å–µ–≥–¥–∞ "the" –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Å—Ç
        if self.preposition_rules:
            specific_places = self.preposition_rules.get("article_after_preposition", {}).get("specific_places", [])
            if any(place in word_lower for place in ["–ø–∞—Ä–∫", "–æ—Ñ–∏—Å", "—Å–∞–¥", "–¥–µ—Ä–µ–≤–Ω", "—Å—Ç–æ–ª", "–±–∏–±–ª–∏–æ—Ç–µ–∫"]):
                return "the"

        # –ò—Å—á–∏—Å–ª—è–µ–º—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ ‚Äî "a"
        if word_lower in ["–¥–æ–º", "–∫–Ω–∏–≥–∞", "–≥–∞–∑–µ—Ç–∞", "—Å–æ–±–∞–∫–∞", "—è–±–ª–æ–∫–æ", "–ø–∞–ø–∞", "–º–∞–º–∞"]:
            return "a"

        return "a"

    def determine_sentence_pattern(self, structure: Dict, sentence: str) -> str:
        return self.detect_special_pattern(sentence, structure)

    def parse_russian_sentence_improved(self, sentence: str) -> Dict:
        original = sentence.strip()
        words = []

        if self.STANZA_AVAILABLE:
            stanza_analysis = self.stanza_syntax_analysis(original)
            if "error" not in stanza_analysis:
                for sent in stanza_analysis.get("sentences", []):
                    for word in sent.get("words", []):
                        words.append(word["text"])
            else:
                clean = re.sub(r'[^\w\s]', ' ', original)
                words = [w for w in clean.strip().split() if w]
        else:
            clean = re.sub(r'[^\w\s]', ' ', original)
            words = [w for w in clean.strip().split() if w]

        punctuation_marks = {'?', '!', '.', ',', ';', ':', '"', "'", '¬ª', '¬´', '(', ')'}
        words = [w for w in words if w not in punctuation_marks]

        word_translations = {}
        for word in words:
            word_translations[word] = self.get_word_translation_improved(word)

        structure = {
            "original": original,
            "words_ru": words,
            "words_en": [word_translations[word] for word in words],
            "word_translations": word_translations,
            "type": self.detect_sentence_type(original),
            "subject": None,
            "verb": None,
            "object": None,
            "indirect_object": None,
            "verb_tense": None,
            "adverbs": [],
            "question_word": None,
            "punctuation": self.extract_punctuation(original),
            "stanza_used": self.STANZA_AVAILABLE,
            "morph_used": self.MORPH_AVAILABLE
        }

        self.advanced_syntax_analysis_with_stanza(structure, original)

        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        if structure["type"] == "interrogative" and not structure.get("question_word"):
            question_words = {'—á—Ç–æ', '–∫—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–∞–∫', '—Å–∫–æ–ª—å–∫–æ'}
            for word in structure["words_ru"]:
                if word.lower() in question_words:
                    structure["question_word"] = word
                    break
        if structure["type"] == "interrogative" and "—Å–∫–æ–ª—å–∫–æ" in sentence.lower():
            # –†–∞–∑–±–∏—Ä–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç/—Å—Ç–æ—è—Ç X?"
            words_lower = [w.lower() for w in structure["words_ru"]]

            if "—Å–∫–æ–ª—å–∫–æ" in words_lower:
                —Å–∫–æ–ª—å–∫–æ_index = words_lower.index("—Å–∫–æ–ª—å–∫–æ")

                # –ò—â–µ–º –≥–ª–∞–≥–æ–ª "—Å—Ç–æ–∏—Ç" –∏–ª–∏ "—Å—Ç–æ—è—Ç"
                for i in range(—Å–∫–æ–ª—å–∫–æ_index + 1, len(structure["words_ru"])):
                    word_lower = structure["words_ru"][i].lower()
                    if word_lower in ["—Å—Ç–æ–∏—Ç", "—Å—Ç–æ—è—Ç"]:
                        # –°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –ø–æ—Å–ª–µ –≥–ª–∞–≥–æ–ª–∞ - —ç—Ç–æ –æ–±—ä–µ–∫—Ç
                        if i + 1 < len(structure["words_ru"]):
                            next_word = structure["words_ru"][i + 1]
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                            if next_word not in ['?', '!', '.', ',']:
                                structure["object"] = next_word
                        break

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Å–≤–µ–Ω–Ω–æ–≥–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
        if structure["verb"]:
            verb_clean = structure["verb"].lower()
            dative_verbs = {"–¥–∞–ª", "–¥–∞–ª–∞", "–¥–∞–ª–æ", "–¥–∞–ª–∏", "–¥–∞—Ç—å", "–ø–æ–¥–∞—Ä–∏–ª", "–ø–æ–¥–∞—Ä–∏–ª–∞", "–æ—Ç–¥–∞–ª", "–æ—Ç–¥–∞–ª–∞", "–ø–µ—Ä–µ–¥–∞–ª",
                            "–ø–µ—Ä–µ–¥–∞–ª–∞"}
            if verb_clean in dative_verbs:
                verb_index = None
                for i, word in enumerate(structure["words_ru"]):
                    if word == structure["verb"]:
                        verb_index = i
                        break
                candidates = []
                if verb_index is not None:
                    for i in range(verb_index + 1, len(structure["words_ru"])):
                        word = structure["words_ru"][i]
                        if word == structure.get("object"):
                            continue
                        candidates.append(word)
                if structure.get("object") and len(candidates) >= 1:
                    structure["indirect_object"] = candidates[0]

        if structure["verb"]:
            verb_info = self.get_verb_info(structure["verb"])
            structure["verb_tense"] = verb_info["tense"]
            verb_inf_en = self._safe_translate_word(verb_info["infinitive"])
            structure["word_translations"][structure["verb"]] = verb_inf_en
            structure["words_en"] = [structure["word_translations"].get(w, w) for w in structure["words_ru"]]

        structure["pattern"] = self.determine_sentence_pattern(structure, original)
        return structure

    def detect_sentence_type(self, sentence: str) -> str:
        if '?' in sentence:
            return "interrogative"
        elif '!' in sentence:
            return "exclamative"
        else:
            return "declarative"

    def extract_punctuation(self, sentence: str) -> str:
        if sentence.endswith('?'):
            return '?'
        elif sentence.endswith('!'):
            return '!'
        else:
            return '.'

    def transform_sentence_structure(self, ru_structure: Dict) -> Dict:
        en_structure = ru_structure.copy()
        en_structure["rules_source"] = "JSON —Ñ–∞–π–ª" if self.json_loaded else "–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞"
        transformations = []

        if en_structure["type"] == "interrogative":
            en_structure["word_order"] = "question_word + auxiliary + subject + verb"
            transformations.append("–ò–Ω–≤–µ—Ä—Å–∏—è –≤–æ–ø—Ä–æ—Å–∞")
        else:
            transformations.append(f"–ü—Ä–∏–º–µ–Ω—ë–Ω —à–∞–±–ª–æ–Ω: {en_structure['pattern']}")

        en_structure["transformations"] = transformations
        return en_structure

    def build_improved_question(self, en_structure: Dict) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        words = []
        question_word = en_structure.get("question_word", "").lower()

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è "—Å–∫–æ–ª—å–∫–æ"
        if question_word == "—Å–∫–æ–ª—å–∫–æ":
            pattern = self.detect_special_pattern(en_structure["original"], en_structure)

            if pattern == "PRICE_QUESTION":
                # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
                obj_ru = en_structure.get("object")

                # –°–ø–æ—Å–æ–± 1: –ò–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                if not obj_ru:
                    # –°–ø–æ—Å–æ–± 2: –ò–∑ –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–≤
                    words_ru = en_structure.get("words_ru", [])
                    for i, word in enumerate(words_ru):
                        if word.lower() == "—Å–∫–æ–ª—å–∫–æ" and i + 2 < len(words_ru):
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º "—Å—Ç–æ–∏—Ç/—Å—Ç–æ—è—Ç"
                            obj_ru = words_ru[i + 2]
                            break

                # –°–ø–æ—Å–æ–± 3: –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å–ª–µ –≥–ª–∞–≥–æ–ª–∞
                if not obj_ru:
                    verb_found = False
                    words_ru = en_structure.get("words_ru", [])
                    for word in words_ru:
                        if word.lower() in ["—Å—Ç–æ–∏—Ç", "—Å—Ç–æ—è—Ç"]:
                            verb_found = True
                            continue
                        if verb_found and word not in ["—Å–∫–æ–ª—å–∫–æ", "?"]:
                            obj_ru = word
                            break

                # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
                if not obj_ru:
                    return "How much does it cost?"

                obj_ru_lower = obj_ru.lower()

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–±—ä–µ–∫—Ç–∞
                uncountable_nouns = {"–∫–æ—Ñ–µ", "—á–∞–π", "–º–æ–ª–æ–∫–æ", "–≤–æ–¥–∞", "—Å–∞—Ö–∞—Ä", "—Å–æ–ª—å", "—Ö–ª–µ–±"}
                countable_nouns = {"—è–±–ª–æ–∫–∏", "—è–±–ª–æ–∫–æ", "–∫–Ω–∏–≥–∏", "–∫–Ω–∏–≥–∞", "–≥–∞–∑–µ—Ç—ã", "–≥–∞–∑–µ—Ç–∞", "—Å–æ–±–∞–∫–∏", "—Å–æ–±–∞–∫–∞"}

                # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥
                obj_en = en_structure["word_translations"].get(obj_ru, obj_ru)

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º JSON –ø—Ä–∞–≤–∏–ª–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
                if self.json_loaded and "PRICE_QUESTION" in self.extended_patterns:
                    price_config = self.extended_patterns["PRICE_QUESTION"]
                    question_mapping = price_config.get("question_mapping", {})

                    is_uncountable = any(unc in obj_ru_lower for unc in uncountable_nouns)
                    is_countable = any(cnt in obj_ru_lower for cnt in countable_nouns)

                    if is_uncountable:
                        mapping = question_mapping.get("uncountable", {})
                    elif is_countable:
                        mapping = question_mapping.get("countable", {})
                    else:
                        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
                        mapping = question_mapping.get("uncountable", {})

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
                    q_word = mapping.get("question_word", "How much")
                    auxiliary = mapping.get("auxiliary", "does")
                    verb_word = mapping.get("verb", "cost")

                    # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç –≤ subject –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    subject_text = mapping.get("subject", "{object}")
                    if "{object}" in subject_text:
                        subject_text = subject_text.replace("{object}", obj_en)

                    # –°–æ–±–∏—Ä–∞–µ–º –≤–æ–ø—Ä–æ—Å
                    return f"{q_word} {auxiliary} {subject_text} {verb_word}?"

                # Fallback
                if any(unc in obj_ru_lower for unc in uncountable_nouns):
                    return f"How much does {obj_en} cost?"
                else:
                    return f"How much do {obj_en} cost?"

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        if en_structure.get("question_word"):
            q_ru = en_structure["question_word"].lower()
            q_en = self.correction_dict.get(q_ru, q_ru)
            words.append(q_en.capitalize())

        subject_ru = en_structure.get("subject", "")
        subj_lower = subject_ru.lower() if subject_ru else ""

        # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –≥–ª–∞–≥–æ–ª
        if subj_lower in ["–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ", "–ø–∞–ø–∞", "–º–∞–º–∞"]:
            auxiliary = "does"
        else:
            auxiliary = "do"

        words.append(auxiliary)

        if subject_ru:
            subj_en = self.correction_dict.get(
                subj_lower,
                en_structure["word_translations"].get(subject_ru, subject_ru)
            )
            words.append(subj_en)

        if en_structure.get("verb"):
            verb_base = en_structure["word_translations"].get(
                en_structure["verb"],
                en_structure["verb"]
            )
            words.append(verb_base)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        to_skip = set()
        if en_structure.get("question_word"):
            to_skip.add(en_structure["question_word"].lower())
        if en_structure.get("subject"):
            to_skip.add(en_structure["subject"].lower())
        if en_structure.get("verb"):
            to_skip.add(en_structure["verb"].lower())

        for word_ru in en_structure.get("words_ru", []):
            if word_ru.lower() in to_skip:
                continue

            if word_ru == en_structure.get("object"):
                article = self.determine_article(en_structure, word_ru)
                if article:
                    words.append(article)
                word_en = en_structure["word_translations"].get(word_ru, word_ru)
                words.append(word_en)
            else:
                word_en = en_structure["word_translations"].get(word_ru, word_ru)
                words.append(word_en)

        return " ".join(words) + "?"

    def build_improved_statement(self, en_structure: Dict) -> str:
        pattern = en_structure.get("pattern", "SVO")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –∏–∑ JSON
        if self.json_loaded and self.extended_patterns and pattern in self.extended_patterns:
            return self.generate_from_json_template(en_structure, pattern)

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
        return self.generate_from_basic_pattern(en_structure, pattern)

    def generate_from_json_template(self, en_structure: Dict, pattern: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Ç—Ä–æ–≥–æ –ø–æ —à–∞–±–ª–æ–Ω—É –∏–∑ JSON"""
        template_config = self.extended_patterns[pattern]
        template = template_config.get("template", "{subject} {verb}")

        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        components = {}

        # –ü–æ–¥–ª–µ–∂–∞—â–µ–µ –∏ –≥–ª–∞–≥–æ–ª
        if pattern == "POSSESSION":
            subject_ru = en_structure.get("subject", "").lower()
            subject_data = template_config["subject_mapping"].get(subject_ru, {"subject": subject_ru, "verb": "have"})
            components["subject"] = subject_data["subject"]
            components["verb"] = subject_data["verb"]

            # –û–±—ä–µ–∫—Ç
            obj_ru = en_structure.get("object", "")
            obj_en = en_structure["word_translations"].get(obj_ru, obj_ru)
            article = self.determine_article(en_structure, obj_ru)
            components["object"] = (article + " " + obj_en).strip() if article else obj_en

            # –ú–µ—Å—Ç–æ
            location = ""
            for word_ru in en_structure["words_ru"]:
                if word_ru.lower() in ["–≤", "–≤–æ", "–Ω–∞", "—É"] and word_ru != obj_ru:
                    idx = en_structure["words_ru"].index(word_ru)
                    if idx + 1 < len(en_structure["words_ru"]):
                        place_ru = en_structure["words_ru"][idx + 1]
                        place_en = en_structure["word_translations"].get(place_ru, place_ru)
                        location = f" in the {place_en}"
            components["location"] = location

            try:
                result = template.format(**components)
                return result + en_structure["punctuation"]
            except KeyError as e:
                pass

        elif pattern == "PHRASAL_VERB":
            subject_ru = en_structure.get("subject", "").lower()
            subject_en = self.correction_dict.get(subject_ru,
                                                  en_structure["word_translations"].get(en_structure.get("subject", ""),
                                                                                        subject_ru))
            components["subject"] = subject_en.capitalize()

            verb_ru = en_structure.get("verb", "").lower()
            verb_mapping = template_config.get("verb_mapping", {})
            verb_data = verb_mapping.get(verb_ru, {"verb_phrase": verb_ru})
            components["verb_phrase"] = verb_data["verb_phrase"]

            obj_ru = en_structure.get("object", "")
            components["object"] = en_structure["word_translations"].get(obj_ru, obj_ru)

            # –ú–µ—Å—Ç–æ
            location = ""
            for word_ru in en_structure["words_ru"]:
                if word_ru.lower() in ["–≤", "–≤–æ", "–Ω–∞", "—É"] and word_ru != obj_ru:
                    idx = en_structure["words_ru"].index(word_ru)
                    if idx + 1 < len(en_structure["words_ru"]):
                        place_ru = en_structure["words_ru"][idx + 1]
                        place_en = en_structure["word_translations"].get(place_ru, place_ru)
                        location = f" in the {place_en}"
            components["location"] = location

            try:
                result = template.format(**components)
                return result + en_structure["punctuation"]
            except KeyError as e:
                pass

        # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, fallback
        return self.generate_from_basic_pattern(en_structure, pattern)

    def generate_from_basic_pattern(self, en_structure: Dict, pattern: str) -> str:
        words = []
        if en_structure.get("subject"):
            subj_ru = en_structure["subject"].lower()
            subj_en = self.correction_dict.get(subj_ru, en_structure["word_translations"].get(en_structure["subject"],
                                                                                              en_structure["subject"]))
            words.append(subj_en.capitalize())
        if en_structure.get("verb"):
            verb_base = en_structure["word_translations"][en_structure["verb"]]
            # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –¥–ª—è 3-–≥–æ –ª–∏—Ü–∞
            subject_ru = en_structure.get("subject", "").lower()
            if subject_ru in ["–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ", "–ø–∞–ø–∞", "–º–∞–º–∞", "—É—á–∏—Ç–µ–ª—å"]:
                if verb_base == "have":
                    verb_conj = "has"
                elif verb_base == "do":
                    verb_conj = "does"
                else:
                    verb_conj = verb_base + "s"
                words.append(verb_conj)
            else:
                words.append(verb_base)
        if pattern == "SVOO":
            if en_structure.get("indirect_object"):
                words.append(en_structure["word_translations"][en_structure["indirect_object"]])
            if en_structure.get("object"):
                article = self.determine_article(en_structure, en_structure["object"])
                if article:
                    words.append(article)
                words.append(en_structure["word_translations"][en_structure["object"]])
        elif pattern in ["SVOA", "SVO"]:
            if en_structure.get("object"):
                article = self.determine_article(en_structure, en_structure["object"])
                if article:
                    words.append(article)
                words.append(en_structure["word_translations"][en_structure["object"]])
            if pattern == "SVOA":
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞ –≤ –∫–æ–Ω–µ—Ü
                for word_ru in en_structure["words_ru"]:
                    if word_ru in [en_structure.get("subject"), en_structure.get("verb"), en_structure.get("object")]:
                        continue
                    word_en = en_structure["word_translations"][word_ru]
                    if word_ru.lower() not in ["–≤", "–≤–æ", "–Ω–∞", "—É"]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏
                        words.append(word_en)
        return " ".join(words) + en_structure["punctuation"]

    def generate_english_sentence(self, en_structure: Dict) -> str:
        if en_structure["type"] == "interrogative":
            return self.build_improved_question(en_structure)
        else:
            return self.build_improved_statement(en_structure)

    def translate_with_analysis(self, russian_sentence: str) -> Dict:
        ru_structure = self.parse_russian_sentence_improved(russian_sentence)
        en_structure = self.transform_sentence_structure(ru_structure)
        translation = self.generate_english_sentence(en_structure)
        return {
            "original": russian_sentence,
            "translation": translation,
            "word_translations": ru_structure["word_translations"],
            "syntax_transformations": en_structure["transformations"],
            "sentence_type": ru_structure["type"],
            "sentence_pattern": ru_structure["pattern"],
            "rules_source": en_structure.get("rules_source", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
            "sentence_structure": {
                "subject": ru_structure["subject"],
                "verb": ru_structure["verb"],
                "object": ru_structure["object"],
                "indirect_object": ru_structure["indirect_object"],
                "question_word": ru_structure["question_word"]
            },
            "stanza_used": ru_structure["stanza_used"],
            "morph_used": ru_structure["morph_used"]
        }


def demonstrate_stanza_translator():
    translator = AdvancedTransformationalTranslator()
    test_sentences = [
        "–í –ø–∞—Ä–∫–µ —è –≤–∏–∂—É —Å–æ–±–∞–∫—É.",
        "–û–Ω–∞ –ø—å—ë—Ç —á–∞–π —Å –º–æ–ª–æ–∫–æ–º.",
        "–ö–æ–≥–¥–∞ —è –ø—Ä–∏—à–µ–ª –¥–æ–º–æ–π, –º–∞–º–∞ –≥–æ—Ç–æ–≤–∏–ª–∞ —É–∂–∏–Ω.",
        "–£ –º–µ–Ω—è –µ—Å—Ç—å –¥–æ–º –≤ –¥–µ—Ä–µ–≤–Ω–µ.",
        "–Ø –∫–ª–∞–¥—É –∫–Ω–∏–≥—É –Ω–∞ —Å—Ç–æ–ª.",
        "–ö—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –æ—Ñ–∏—Å–µ?",
        "–ß—Ç–æ –¥–µ–ª–∞–µ—Ç –ø–∞–ø–∞ –≤ —Å–∞–¥—É?",
        "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –∫–æ—Ñ–µ?",
        "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ—è—Ç —è–±–ª–æ–∫–∏?",
        "–û–Ω–∏ —á–∏—Ç–∞—é—Ç –≥–∞–∑–µ—Ç—É –∏ –ø—å—é—Ç –≤–æ–¥—É.",
        "–Ø –∏–¥—É –≤ –ø–∞—Ä–∫.",
        "–û–Ω –≤–∏–¥–∏—Ç –±–æ–ª—å—à–æ–π –¥–æ–º.",
        "–Ø –∂–¥—É —Ç–µ–±—è –∑–¥–µ—Å—å.",
        "–ú—ã –Ω–∞—á–∞–ª–∏ —á–∏—Ç–∞—Ç—å –∫–Ω–∏–≥—É –≤—á–µ—Ä–∞."
    ]
    print("\n" + "=" * 70)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø: –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ü–ï–†–ï–í–û–î–ß–ò–ö")
    print("=" * 70)
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\n{i}. üìù –†–£–°–°–ö–ò–ô: {sentence}")
        result = translator.translate_with_analysis(sentence)
        print(f"   üåê –ê–ù–ì–õ–ò–ô–°–ö–ò–ô: {result['translation']}")
        print(f"   üî§ –ü–µ—Ä–µ–≤–æ–¥—ã —Å–ª–æ–≤: {result['word_translations']}")
        print(f"   üèóÔ∏è  –®–∞–±–ª–æ–Ω: {result['sentence_pattern']}")
        print("-" * 70)


if __name__ == "__main__":
    demonstrate_stanza_translator()